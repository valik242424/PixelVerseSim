# PixelVerseSim v2 - RL Edition

Симуляція життя на сітці з ботами, що керуються індивідуальними нейромережами (з GRU для пам'яті) та навчаються за допомогою Reinforcement Learning (DQN).

## Поточний Стан

*   Симуляція запускається з ботами, стінами та їжею на сітці.
*   Боти мають індивідуальні "мізки" на основі GRU, що дозволяє їм мати пам'ять.
*   Реалізовано алгоритм навчання Deep Q-Network (DQN) для кожного бота окремо.
*   Боти отримують винагороду/покарання за поїдання їжі, рух, зіткнення зі стінами та смерть.
*   Використовується Experience Replay Buffer та epsilon-greedy стратегія для балансу дослідження/використання.
*   Реалізовано механізм навчання (оновлення ваг мережі) та оновлення цільової мережі (target network).
*   Додано UI для:
    *   Візуалізації симуляції та переміщення по карті (WASD).
    *   Керування симуляцією (Старт/Стоп/Крок/Скидання/Швидкість).
    *   Ввімкнення/вимкнення режиму тренування для всіх ботів.
    *   Збереження та завантаження навчених моделей для кожного бота окремо.
    *   Перегляду логів та інформації про клітинки/ботів.

## TODO / Плани на Майбутнє

### Покращення RL та Навчання

-   [x] **Розробити функцію винагороди (Reward Function):** *Реалізовано в `simulation_engine.py` (їжа, рух, стіна, смерть).*
-   [x] **Вибрати та реалізувати алгоритм RL:** *Вибрано DQN, реалізовано в `dqn_agent.py`.*
    -   [x] Experience Replay Buffer (для DQN): *Клас `ReplayBuffer` в `dqn_agent.py`.*
    -   [x] Механізм вибору дії (Action Selection): *Epsilon-greedy в `DQNAgent.select_action`.*
    -   [x] Цикл навчання: *Метод `DQNAgent._learn`.*
-   [x] **Інтегрувати цикл навчання в симуляцію:** *Виконується через `bot.store_experience` в `simulation_engine.py`.*
    -   [x] Додати окремий режим "Training Mode": *Реалізовано через чекбокс в UI та `agent/engine.set_training_mode`.*
    -   [x] Визначити, коли і як часто запускати оновлення мережі: *Контролюється `LEARN_EVERY_N_STEPS` та `TARGET_UPDATE_INTERVAL` в `config.py`.*
-   [x] **Збереження та завантаження навчених моделей:** *Реалізовано в `DQNAgent` та `SimulationEngine`, доступно через UI.*
-   [x] **Додати елементи керування навчанням в UI:**
    -   [x] Кнопки/чекбокс "Training Mode".
    -   [x] Кнопки "Save Models", "Load Models".
    -   [ ] **Відображення прогресу навчання:** Додати графіки або показники в UI (наприклад, середня винагорода за останні N кроків/епізодів, значення функції втрат).
-   [ ] **Покращити стабільність/ефективність навчання:**
    -   Розглянути варіанти покращення DQN (Double DQN, Dueling DQN).
    -   Дослідити кращі способи обробки стану GRU під час навчання з буфера (замість поточного спрощення). Можливо, перейти на DRQN (Deep Recurrent Q-Network).
    -   Підбір гіперпараметрів (`config.py`) для кращих результатів.

### Покращення Симуляції та UI

-   [x] **Додати Сутність 'Food':** *Реалізовано.*
-   [ ] **Покращити логування/візуалізацію:**
    -   Додати більше візуальної інформації про стан ботів (наприклад, відображати рівень енергії над ботом?).
    -   Зробити логи менш спамними під час швидкої симуляції (фільтрувати або агрегувати).
-   [ ] **Оптимізація:**
    -   Профілювати код, знайти вузькі місця (особливо цикл навчання `_learn` та обробка подій в `simulation_engine.step`).
    -   Оптимізувати рендеринг (`SimulationGridWidget`), якщо буде гальмувати на великих сітках/з великою кількістю ботів.
-   [ ] **Додати більше взаємодій/сутностей:**
    -   Можливо, взаємодія між ботами (атака, співпраця?).
    -   Інші типи ресурсів або перешкод.
    -   "Органіка" на місці померлих ботів як джерело енергії?
-   [ ] **Рефакторинг/Чистка коду:** Завжди є що покращити!

## Як Запустити

1.  Переконайся, що встановлено Python та необхідні бібліотеки:
    ```bash
    pip install PySide6 torch numpy # (numpy може знадобитись неявно)
    ```
2.  Запусти головний скрипт:
    ```bash
    python main.py
    ```
3.  Використовуй UI для керування симуляцією та навчанням. Моделі зберігаються/завантажуються з папки `models/` за замовчуванням.