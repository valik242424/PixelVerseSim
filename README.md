# PixelVerseSim

Симуляція життя на сітці з ботами, що (в майбутньому) керуються нейромережами.

## TODO / Плани на Майбутнє

### Навчання з Підкріпленням (Reinforcement Learning - RL)

Найближчий великий крок - навчити мізки ботів!

-   [ ] **Розробити функцію винагороди (Reward Function):** Визначити, за що боти отримуватимуть "плюшки", а за що "по шапці".
    -   Приклади: + за знаходження їжі (якщо додати), - за витрачену енергію (за кожен крок), - за зіткнення зі стіною/іншим ботом?, + за виживання протягом певного часу?
-   [ ] **Вибрати та реалізувати алгоритм RL:**
    -   Розглянути: Deep Q-Network (DQN) як хороший старт. Можливо, пізніше PPO або інші.
    -   Реалізувати ключові компоненти:
        -   [ ] Experience Replay Buffer (для DQN): Зберігання переходів `(стан, дія, винагорода, наступний_стан)`.
        -   [ ] Механізм вибору дії (Action Selection): Збалансувати exploration (випадкові дії для дослідження) та exploitation (використання найкращої відомої дії) - наприклад, epsilon-greedy.
        -   [ ] Цикл навчання: Періодичне семплування батчів з буфера та оновлення ваг мережі (backpropagation).
-   [ ] **Інтегрувати цикл навчання в симуляцію:**
    -   Можливо, додати окремий режим "Training Mode".
    -   Визначити, коли і як часто запускати оновлення мережі.
-   [ ] **Збереження та завантаження навчених моделей:** Додати функціонал для збереження стану навченої мережі (`state_dict`) у файл та завантаження її для використання або подальшого навчання.
-   [ ] **Додати елементи керування навчанням в UI:**
    -   Кнопки "Start Training", "Stop Training".
    -   Кнопки/поля для "Save Model", "Load Model".
    -   Відображення прогресу навчання (середня винагорода, втрати тощо).

### Інші Покращення

-   [ ] **Додати Сутність 'Food':** Простий об'єкт, який боти можуть "з'їдати", щоб відновити енергію. Це зробить навчання більш осмисленим.
-   [ ] **Покращити логування/візуалізацію:** Відображати більше інформації про стан ботів (енергію, можливо, останню дію) або про процес навчання.
-   [ ] **Оптимізація:** Пошук вузьких місць у продуктивності, особливо під час навчання з великою кількістю ботів/кроків.
-   [ ] **Налаштування параметрів:** Підбір гіперпараметрів мережі та алгоритму навчання (learning rate, розмір буфера, epsilon decay тощо).